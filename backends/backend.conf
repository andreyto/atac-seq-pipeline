include required(classpath("application"))

backend {
  default = "Local"
  providers {

    Local {
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
      config {
        concurrent-job-limit = 6
      }
    }

    sge {
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
      config {
        concurrent-job-limit = 6
        runtime-attributes = """
        String sge_pe = "shm"
        Int cpu = 1
        Int? time
        Int? memory_mb
        String? sge_queue
        """
        submit = """
        qsub \
        -terse \
        -V \
        -b n \
        -N ${job_name} \
        -wd ${cwd} \
        -o ${out} \
        -e ${err} \
        ${if cpu>1 then "-pe " + sge_pe + " " + cpu else " "} \
        ${"-l h_vmem=" + memory_mb/cpu + "m"} \
        ${"-l s_vmem=" + memory_mb/cpu + "m"} \
        ${"-l h_rt=" + time*3600} \
        ${"-l s_rt=" + time*3600} \
        ${"-q " + sge_queue} \
        ${script}
        """
        kill = "qdel ${job_id}"
        check-alive = "qstat -j ${job_id}"
        job-id-regex = "(\\d+)"
      }
    }

    slurm {
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
      config {
        concurrent-job-limit = 6
        runtime-attributes = """
        Int cpu = 1
        Int? time
        Int? memory_mb
        String? slurm_partition
        """
        submit = """
        sbatch \
        --export=ALL \
        -J ${job_name} \
        -D ${cwd} \
        -o ${out} \
        -e ${err} \
        ${"-t " + time*60} \
        -n 1 \
        --ntasks-per-node=1 \
        ${"--cpus-per-task=" + cpu} \
        ${"--mem=" + memory_mb} \
        ${"-p " + slurm_partition} \
        --wrap "/bin/bash ${script}"
        """
        kill = "scancel ${job_id}"
        check-alive = "squeue -j ${job_id}"
        job-id-regex = "Submitted batch job (\\d+).*"
      }
    }

    google {
      actor-factory = "cromwell.backend.impl.jes.JesBackendLifecycleActorFactory"
      config {
        # Google project
        project = "your-project-name"
    
        # Base bucket for workflow executions
        root = "gs://your-bucket-name"

        concurrent-job-limit = 1000
        genomics-api-queries-per-100-seconds = 1000
        maximum-polling-interval = 600

        genomics {
          auth = "application-default"
          compute-service-account = "default"
          endpoint-url = "https://genomics.googleapis.com/"
          restrict-metadata-access = false
        }

        filesystems {
          gcs {
            auth = "application-default"
          }
        }
      }
    }
    
    pbs {
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
      config {
    
        # Limits the number of concurrent jobs
        concurrent-job-limit = 500
    
        runtime-attributes = """
        Int cpu = 1
        Int? memory_gb
        String? pbs_queue
        String? pbs_account
        String? conda_env
        """
    
        submit = """
        touch ${out} ${err}
        if [ -n "${conda_env}" ]; then
        script_new=${script+".new"}
        (
        cat <<'EOF'
        #!/bin/sh
        set -ex
        ## Cromwell must be running from some Conda environment to detect
        ## what Conda install to use. Conda env var is dereferenced on the
        ## server side and written into the job script
        [ -n "$CONDA_PREFIX_1" ] || exit 1
        . $CONDA_PREFIX_1/etc/profile.d/conda.sh
        ${"conda activate " + conda_env}
        EOF
        ) > "$script_new"
            echo "hostname; date; printenv" >> "$script_new"
            cat ${script} >> "$script_new"
            mv "$script_new" ${script}
        fi
        qsub \
        -S /bin/sh \
        -V \
        -N ${job_name} \
        -d ${cwd} \
        -k n \
        -m n \
        -o ${out} \
        -e ${err} \
        ${"-l nodes=1:ppn=" + cpu} \
        ${"-l mem=" + memory_gb + "gb"} \
        ${"-q " + pbs_queue} \
        ${"-A " + pbs_account} \
        ${script}
        """
    
        kill = "qdel ${job_id}"
        check-alive = "qstat ${job_id}"
        job-id-regex = "(.*)"
      filesystems = {
        local {
            localization: [
            "hard-link", "soft-link", "copy"
            ]
            caching {
              # When copying a cached result, what type of file duplication should occur. Attempted in the order listed below:
              duplication-strategy: [
                "hard-link", "soft-link", "copy"
              ]

              # Possible values: file, path
              # "file" will compute an md5 hash of the file content.
              # "path" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to "soft-link",
              # in order to allow for the original file path to be hashed.
              hashing-strategy: "file"

              # When true, will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash.
              # If false or the md5 does not exist, will proceed with the above-defined hashing strategy.
              check-sibling-md5: false
            }
        }
#        gcs {
#            # References an auth scheme defined in the 'google' stanza.
#            auth = "application-default"
#        }
      }
      }
    }
  }
}

system {
  abort-jobs-on-terminate = true
  graceful-server-shutdown = true
}

call-caching {
  enabled = false
  invalidate-bad-cache-results = true
}

google {
  application-name = "cromwell"
  auths = [
    {
      name = "application-default"
      scheme = "application_default"
    }
  ]
}
